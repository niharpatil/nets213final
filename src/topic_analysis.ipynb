{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Analysis\n",
    "This notbeook contains all of the code for aggregation of topics from the first two HITS. It takes in the data from the first 2 HITs, and then outputs the overall topic knowledge of mTurk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hit1 = pd.read_csv('hit_1_final_output.csv')\n",
    "hit2 = pd.read_csv('hit_2_output.csv')\n",
    "\n",
    "#Get all of the words that were submitted for the first HIT\n",
    "word_list = []\n",
    "for index, row in hit1.iterrows():\n",
    "    for ans in range(1,5):\n",
    "        word = row['Answer.topic_'+str(ans)]\n",
    "        if isinstance(word, str):\n",
    "            word_list.append(word)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIT1 Data Analysis\n",
    "Here we have all of the word submitted from HIT1 in the variable word_list. With these words, we will run word2Vec and k-means clustering. Looking at the center of each of those clusters, this should give us an idea of what topics Turkers know about in the aggregate. Many words from the first HIT will not be present in word2Vec, which we ignore for now. Those words will be looked at again in HIT 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-47bcfb18c4a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = word2vec.KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "unknowns = []\n",
    "knowns = []\n",
    "knames = []\n",
    "#word_list = dic_list\n",
    "for i in range(len(word_list)):\n",
    "    try:\n",
    "        knowns.append(model[word_list[i]])\n",
    "        knames.append(word_list[i])\n",
    "    except:\n",
    "        unknowns.append(word_list[i])\n",
    "    for j in range(len(word_list)):\n",
    "        if word_list[j] not in unknowns:\n",
    "            try:\n",
    "                print (str(word_list[i])+ \", \"+str(word_list[j])+ \": \" + str(model.similarity(word_list[i],word_list[j])))\n",
    "            except:\n",
    "                unknowns.append(word_list[j])\n",
    "\n",
    "dists = []\n",
    "K = range(1,25)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(knowns)\n",
    "    dists.append(km.inertia_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=np.argmin(dists), random_state=0).fit(knowns)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these clusters, we made the categories used for HIT 2. We posted a HIT which asked turkers to classify all of the words which word2vec could not idenfity, and asked them to classify it as one of the topics which were present from our clustering analysis. We also added an other and don't know option in case the turkers didn't know what the word was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating HIT 2 and HIT 1 into Turker Topic Analysis\n",
    "Here we combine the results from HIT 1 and HIT 2 in order to get a full concept of what topics turkers know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "K = range(1,30)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(knowns)\n",
    "    dists.append(km.inertia_)\n",
    "\n",
    "kmeans = KMeans(n_clusters=np.argmin(dists), random_state=0).fit(knowns)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Psychology', 0.8638474941253662), ('Biology', 0.78260338306427), ('Sociology', 0.7774052023887634)]\n",
      "[('Gardening', 0.9782292246818542), ('gardening', 0.8355257511138916), ('vegetable_gardening', 0.7094038128852844)]\n",
      "[('Basketball', 0.9838793277740479), ('Volleyball', 0.7359228134155273), ('Softball', 0.6746788024902344)]\n",
      "[('Computers', 0.9329081773757935), ('Computer', 0.7407127618789673), ('computers', 0.7210654616355896)]\n",
      "[('Football', 0.9206256866455078), ('Soccer', 0.759657621383667), ('Sports', 0.7445812821388245)]\n",
      "[('Books', 0.9537973403930664), ('Comics', 0.6984010934829712), ('Bookshop', 0.6527296304702759)]\n",
      "[('quilting', 0.8138535022735596), ('crochet', 0.8109740018844604), ('Crochet', 0.8107221126556396)]\n",
      "[('Dogs', 0.8305302858352661), ('Animals', 0.802953839302063), ('animals', 0.7576248645782471)]\n",
      "[('Art', 0.8011147975921631), ('Poetry', 0.7656776905059814), ('Printmaking', 0.7116436958312988)]\n",
      "[('dol##.net_index###.html_http_dol##.net', 0.721744954586029), ('http_dol##.net_index###.html_http', 0.7194985151290894), ('http_dol##.net_index####.html_http', 0.7171704769134521)]\n",
      "[('Music', 0.8682034611701965), ('music', 0.8157165050506592), ('Guitar', 0.6754887104034424)]\n",
      "[('Math', 0.9079212546348572), ('math', 0.821898877620697), ('mathematics', 0.7514215707778931)]\n",
      "[('Cooking', 0.9479556083679199), ('cooking', 0.8826708197593689), ('cook', 0.690744936466217)]\n",
      "[('basketball', 0.8709017038345337), ('soccer', 0.8153940439224243), ('football', 0.8110343217849731)]\n",
      "[('Photography', 0.927348256111145), ('photography', 0.7259604930877686), ('Photographic', 0.6689215302467346)]\n",
      "[('Hiking', 1.0), ('hiking', 0.7093385457992554), ('Hike', 0.6814872026443481)]\n",
      "[('WHOLE_NEW', 0.8512703776359558), ('GEARING_UP_FOR', 0.8479671478271484), ('PICK_YOUR', 0.8478443622589111)]\n",
      "[('Finance', 0.8225609064102173), ('Banking', 0.6772512197494507), ('Financial_Services', 0.6222540736198425)]\n",
      "[('Parenting', 0.9797844886779785), ('parenting', 0.7445102334022522), ('Mothering', 0.6768010258674622)]\n",
      "[('Baseball', 0.9810267686843872), ('baseball', 0.849311351776123), ('MLB', 0.7304302453994751)]\n",
      "[('Education', 0.8045122027397156), ('Teaching', 0.7802101969718933), ('Educations', 0.6979876756668091)]\n",
      "[('Soccer', 0.9441465139389038), ('Hockey', 0.778718113899231), ('Football', 0.7526614665985107)]\n",
      "[('Investing', 0.8900259733200073), ('investing', 0.7400734424591064), ('Harry_Domash_Online', 0.6529433727264404)]\n",
      "[('Fishing', 0.9451600313186646), ('fishing', 0.7352177500724792), ('Sailing', 0.7158492803573608)]\n",
      "[('Nutrition', 0.8624899387359619), ('Medicine', 0.7380903959274292), ('nutrition', 0.7000079154968262)]\n",
      "[('Reddit', 0.9541704654693604), ('Wordpress', 0.7503008246421814), ('Digg', 0.6925498247146606)]\n",
      "[('Movies', 0.982811450958252), ('Movie', 0.7485084533691406), ('movies', 0.6784780621528625)]\n",
      "[('memorizing_vocabulary', 0.670893669128418), ('grammar_vocabulary', 0.6395058035850525), ('conjugating_verbs', 0.6305262446403503)]\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "top_words = []\n",
    "for c in range(np.argmin(dists)):\n",
    "    cluster_words = []\n",
    "    for i in range(len(knowns)):\n",
    "        if(kmeans.labels_[i] == c):\n",
    "            cluster_words.append(knowns[i])    \n",
    "    top3 = model.most_similar(positive=cluster_words, topn=3)\n",
    "    top_words.append(top3[0])\n",
    "    print(top3)\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unknowns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Psychology', 0.8638474941253662),\n",
       " ('Gardening', 0.9782292246818542),\n",
       " ('Basketball', 0.9838793277740479),\n",
       " ('Computers', 0.9329081773757935),\n",
       " ('Football', 0.9206256866455078),\n",
       " ('Books', 0.9537973403930664),\n",
       " ('quilting', 0.8138535022735596),\n",
       " ('Dogs', 0.8305302858352661),\n",
       " ('Art', 0.8011147975921631),\n",
       " ('dol##.net_index###.html_http_dol##.net', 0.721744954586029),\n",
       " ('Music', 0.8682034611701965),\n",
       " ('Math', 0.9079212546348572),\n",
       " ('Cooking', 0.9479556083679199),\n",
       " ('basketball', 0.8709017038345337),\n",
       " ('Photography', 0.927348256111145),\n",
       " ('Hiking', 1.0),\n",
       " ('WHOLE_NEW', 0.8512703776359558),\n",
       " ('Finance', 0.8225609064102173),\n",
       " ('Parenting', 0.9797844886779785),\n",
       " ('Baseball', 0.9810267686843872),\n",
       " ('Education', 0.8045122027397156),\n",
       " ('Soccer', 0.9441465139389038),\n",
       " ('Investing', 0.8900259733200073),\n",
       " ('Fishing', 0.9451600313186646),\n",
       " ('Nutrition', 0.8624899387359619),\n",
       " ('Reddit', 0.9541704654693604),\n",
       " ('Movies', 0.982811450958252),\n",
       " ('memorizing_vocabulary', 0.670893669128418)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_cor = [0] * np.argmin(dists)\n",
    "avg_cor = [0] * np.argmin(dists)\n",
    "n_clust = [0] * np.argmin(dists)\n",
    "for i in range(len(knowns)):\n",
    "    ilabel = kmeans.labels_[i]\n",
    "    tot_cor[ilabel] += model.similarity(knames[i], top_words[ilabel][0])\n",
    "    n_clust[ilabel] += 1\n",
    "for i in range(np.argmin(dists)):\n",
    "    avg_cor[i] = (top_words[i][0],tot_cor[i] / n_clust[i], n_clust[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First number is correlation, second number is the number in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Psychology', 0.5902997267246246, 25),\n",
       " ('Gardening', 0.8739158287644386, 8),\n",
       " ('Basketball', 0.8944290369749069, 10),\n",
       " ('Computers', 0.6778482715288798, 15),\n",
       " ('Football', 0.7235963066418966, 15),\n",
       " ('Books', 0.7705943385759989, 9),\n",
       " ('quilting', 0.64513512807233, 14),\n",
       " ('Dogs', 0.6069824719115308, 19),\n",
       " ('Art', 0.5185368433594704, 24),\n",
       " ('dol##.net_index###.html_http_dol##.net', 0.22521802884037212, 59),\n",
       " ('Music', 0.5956941715752085, 24),\n",
       " ('Math', 0.6399184749885038, 11),\n",
       " ('Cooking', 0.7989143297076226, 20),\n",
       " ('basketball', 0.6462114139607078, 19),\n",
       " ('Photography', 0.7102544486522675, 15),\n",
       " ('Hiking', 0.9999999403953552, 3),\n",
       " ('WHOLE_NEW', 0.6414718429247538, 9),\n",
       " ('Finance', 0.48592983434597653, 18),\n",
       " ('Parenting', 0.8735541755502875, 11),\n",
       " ('Baseball', 0.8824196265024298, 17),\n",
       " ('Education', 0.4810661762952805, 15),\n",
       " ('Soccer', 0.8205217309296131, 16),\n",
       " ('Investing', 0.7309124916791916, 4),\n",
       " ('Fishing', 0.8161789874235789, 3),\n",
       " ('Nutrition', 0.6005270860411904, 11),\n",
       " ('Reddit', 0.8393574953079224, 3),\n",
       " ('Movies', 0.905191159248352, 5),\n",
       " ('memorizing_vocabulary', 0.3493073118083617, 34)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_clust = []\n",
    "rnames = []\n",
    "for i in range(len(knowns)):\n",
    "    if(kmeans.labels_[i] == 10):\n",
    "        random_clust.append(knowns[i])  \n",
    "        rnames.append(knames[i])\n",
    "\n",
    "rdists = []\n",
    "rK = range(1,5)\n",
    "for k in rK:\n",
    "    rkm = KMeans(n_clusters=k).fit(random_clust)\n",
    "    rdists.append(rkm.inertia_)\n",
    "\n",
    "rkmeans = KMeans(n_clusters=np.argmin(rdists), random_state=0).fit(random_clust)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "rtop_words = []\n",
    "for c in range(np.argmin(rdists)):\n",
    "    cluster_words = []\n",
    "    for i in range(len(random_clust)):\n",
    "        if(rkmeans.labels_[i] == c):\n",
    "            cluster_words.append(random_clust[i])    \n",
    "    top3 = model.most_similar(positive=cluster_words, topn=1)\n",
    "    rtop_words.append(top3[0])\n",
    "\n",
    "rtot_cor = [0] * np.argmin(rdists)\n",
    "ravg_cor = [0] * np.argmin(rdists)\n",
    "rn_clust = [0] * np.argmin(rdists)\n",
    "for i in range(len(random_clust)):\n",
    "    ilabel = rkmeans.labels_[i]\n",
    "    rtot_cor[ilabel] += model.similarity(rnames[i], rtop_words[ilabel][0])\n",
    "    rn_clust[ilabel] += 1\n",
    "for i in range(np.argmin(rdists)):\n",
    "    ravg_cor[i] = (rtop_words[i][0],rtot_cor[i] / rn_clust[i], rn_clust[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_c = []\n",
    "for i in range(len(knowns)):\n",
    "    if(kmeans.labels_[i] == 9):\n",
    "        rand_c.append(knames[i])\n",
    "\n",
    "x = np.concatenate([rand_c,rand_d])\n",
    "\n",
    "final_data = pd.DataFrame(unknowns)\n",
    "\n",
    "final_data.to_csv(r'just_unknowns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'art': 3.2411288069293094,\n",
       " 'baseball': 1.8943839061190275,\n",
       " 'basketball': 2.112321877619447,\n",
       " 'cooking': 2.8890751606594023,\n",
       " 'dont_know': 1.5982117910030735,\n",
       " 'education': 8.533109807208717,\n",
       " 'finance': 4.235820061469684,\n",
       " 'football': 3.056719754121263,\n",
       " 'gardening': 2.347024308466052,\n",
       " 'hobby': 7.974294495669182,\n",
       " 'literature': 4.498463257893266,\n",
       " 'math': 1.4138027381950267,\n",
       " 'movies': 2.442022911427773,\n",
       " 'music': 4.660519698239732,\n",
       " 'nutrition': 4.3419949706621965,\n",
       " 'other': 10.930427493713328,\n",
       " 'other_sports': 3.3137747974294496,\n",
       " 'parenting': 3.028778988544286,\n",
       " 'pets': 2.905839620005588,\n",
       " 'photography': 1.587035484772283,\n",
       " 'politics': 3.352891869237217,\n",
       " 'religion': 1.7546800782341436,\n",
       " 'science': 3.7999441184688463,\n",
       " 'soccer': 1.7546800782341436,\n",
       " 'technology': 6.543727298127969,\n",
       " 'television': 2.78290025146689,\n",
       " 'video_games': 3.006426376082705}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count is a dictionary mapping from topics to an integer \"points\", essentially meaning how much of mTurk knows about that topic\n",
    "count = {}\n",
    "total = 0\n",
    "for index, row in hit2.iterrows():\n",
    "    #The rows which contain information about expertise and topic\n",
    "    for col in range(49,130):\n",
    "        category = hit2.columns[col]\n",
    "        if row[category] == True:\n",
    "            #p = point of the last period in the column, useful for finding the topic and word number\n",
    "            p = category.rfind(\".\")\n",
    "            topic = category[p+1:]\n",
    "            num = category[p-1:p]\n",
    "            \n",
    "            #If the turker did not select the expertise level we defaulted it to 2\n",
    "            add = 2\n",
    "            if row[\"Answer.expertise_\"+num+\".high\"]:\n",
    "                add = 3\n",
    "            elif  row[\"Answer.expertise_\"+num+\".med\"]:\n",
    "                add = 2\n",
    "            elif  row[\"Answer.expertise_\"+num+\".high\"]:\n",
    "                add = 1\n",
    "            total += add\n",
    "            if topic in count:\n",
    "                count[topic] += add\n",
    "            else:\n",
    "                count[topic] = add\n",
    "\n",
    "#Go through each of the words which we alredy knew from HIT1 and word2vec and assign it the topic which is closest \n",
    "#to it in terms of cosine similiarity \n",
    "for word in knames:\n",
    "    max_topic = ''\n",
    "    max_cs = -1\n",
    "    for topic in count:\n",
    "        #These topics we can't really find similarities to, because word2vec doesn't do a \n",
    "        #great job of describing them\n",
    "        if topic == \"dont_know\" or topic == \"other_sports\" or topic == \"video_games\":\n",
    "            continue\n",
    "        cs = model.similarity(word, topic)\n",
    "        if cs > max_cs:\n",
    "            max_cs = cs\n",
    "            max_topic = topic\n",
    "    #Add 6 to keep it consistent with the previous system. Each word before was added 3 times\n",
    "    #because each task was completed 3 times on Turk, so the average is 6.\n",
    "    count[max_topic] += 6\n",
    "    total += 6\n",
    "\n",
    "#normalize our count dictionary to percentages\n",
    "for topic in count:\n",
    "    count[topic] /= total\n",
    "    count[topic] *= 100\n",
    "\n",
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
